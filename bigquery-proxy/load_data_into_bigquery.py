"""This script takes the annotated catalog JSON and loads it into a BigQuery table."""

import argparse
import collections
import datetime
import gzip
import json
import ijson
import intervaltree
import os
from pprint import pformat
import re
import requests
from google.cloud import bigquery
import pandas as pd
import pyfaidx
import time
import tqdm
import scipy.stats
import sys

from str_analysis.utils.misc_utils import parse_interval
from str_analysis.utils.canonical_repeat_unit import compute_canonical_motif
from str_analysis.utils.eh_catalog_utils import get_variant_catalog_iterator
from str_analysis.utils.file_utils import tee_stdout_and_stderr_to_log_file
from global_constants import BIGQUERY_COLUMNS

PROJECT_ID = "cmg-analysis"
DATASET_ID = "tandem_repeat_explorer"
TABLE_ID = "catalog"

parser = argparse.ArgumentParser(description="Load data into BigQuery from the annotated catalog JSON file.", formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument("-c", "--catalog-path", help="Path to the annotated catalog JSON file",
                    default="~/code/tandem-repeat-catalogs/results__2026-01-12/release_draft_2026-01-12/TRExplorer.repeat_catalog_v2.hg38.1_to_1000bp_motifs.EH.with_annotations.json.gz")
parser.add_argument("-n", type=int, help="Number of records to read from the catalog")
parser.add_argument("-d", "--known-disease-associated-loci",
                    action="append",
                    help="ExpansionHunter catalog (JSON) or BED file with known disease-associated loci. Can be specified multiple times.")
parser.add_argument("--reference-fasta", default="~/hg38.fa", help="Path of reference genome fasta file")
parser.add_argument("--tenk10k-tsv", default="../data-prep/tenk10k_str_mt_rows.reformatted.tsv.gz")
parser.add_argument("--hprc256-tsv", default="../data-prep/hprc_lps.2025_12.grouped_by_locus_and_motif.with_biallelic_histogram.tsv.gz")
parser.add_argument("--aou1027-tsv", default="../data-prep/AoULR_phase1_TRGT_Weisburd_v1_combined.txt.gz")
parser.add_argument("--vamos-tsv", default="../data-prep/vamos_ori_and_eff_motif_columns.tsv.gz", help="TSV with Vamos fields generated by ../data-prep/compute_vamos_columns.py")
parser.add_argument("--repeat-masker-lookup-json", default="../data-prep/hg38.RepeatMasker.lookup.json.gz")
parser.add_argument("--non-coding-annotations-bed", default="../data-prep/ncAnnot.v0.14.jul2024.filtered.bed.gz")
args = parser.parse_args()

# Set up logging to a timestamped file
log_filename = f"load_data_into_bigquery_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
tee_stdout_and_stderr_to_log_file(log_filename)
print(f"Logging output to {log_filename}")


## Define utility functions
def get_json_iterator(content, is_gzipped=False):
    """Helper function to get an ijson iterator from content."""
    if is_gzipped:
        if isinstance(content, bytes):
            content = gzip.decompress(content)
        else:
            content = gzip.open(content, "rb")

    return ijson.items(content, "item", use_float=True)


def population_data_sanity_check(row, dataset_name):
    """Perform consistency checks on the population data for a single locus. Compare the following columns:

    min_allele
    mode_allele
    median
    99th_percentile
    max_allele

    stdev
    unique_alleles
    num_called_alleles
    stdev_rank_by_motif
    stdev_rank_total_number_by_motif
    """

    errors = []
    if row["min_allele"] > row["mode_allele"]:
        errors.append((f"{dataset_name}: min_allele > mode_allele", f"{row['locus_id']}: min_allele == {row['min_allele']} and mode_allele == {row['mode_allele']}"))
    if row["min_allele"] > row["median"]:
        errors.append((f"{dataset_name}: min_allele > median", f"{row['locus_id']}: min_allele == {row['min_allele']} and median == {row['median']}"))
    if row["mode_allele"] > row["max_allele"]:
        errors.append((f"{dataset_name}: mode_allele > max_allele", f"{row['locus_id']}: mode_allele == {row['mode_allele']} and max_allele == {row['max_allele']}"))
    if row["median"] > row["99th_percentile"]:
        errors.append((f"{dataset_name}: median > 99th_percentile", f"{row['locus_id']}: median == {row['median']} and 99thPercentile == {row['99th_percentile']}"))
    if row["99th_percentile"] > row["max_allele"]:
        errors.append((f"{dataset_name}: 99th_percentile > max_allele", f"{row['locus_id']}: 99th_percentile == {row['99th_percentile']} and max_allele == {row['max_allele']}"))

    if row["unique_alleles"] > row["max_allele"] - row["min_allele"] + 1:
        errors.append((f"{dataset_name}: unique_alleles > max_allele - min_allele + 1", f"{row['locus_id']}: min_allele == {row['min_allele']}, max_allele == {row['max_allele']}, and unique_alleles == {row['unique_alleles']}"))

    if row["stdev_rank_by_motif"] > row["stdev_rank_total_number_by_motif"]:
        errors.append((f"{dataset_name}: stdev_rank_by_motif > stdev_rank_total_number_by_motif", f"{row['locus_id']}: stdev_rank_by_motif == {row['stdev_rank_by_motif']} and stdev_rank_total_number_by_motif == {row['stdev_rank_total_number_by_motif']}"))

    allele_set = set([row["min_allele"], row["mode_allele"], row["max_allele"]])
    if row["unique_alleles"] < len(allele_set):
        errors.append((f"{dataset_name}: unique_alleles < len(allele_set)", f"{row['locus_id']}: unique_alleles == {row['unique_alleles']}"))
    if row["num_called_alleles"] < row["unique_alleles"]:
        errors.append((f"{dataset_name}: num_called_alleles < unique_alleles", f"{row['locus_id']}: num_called_alleles == {row['num_called_alleles']}, unique_alleles == {row['unique_alleles']}"))
    if row["stdev"] > 0 and row["num_called_alleles"] == 1:
        errors.append((f"{dataset_name}: stdev > 0 and num_called_alleles == 1", f"{row['locus_id']}: stdev == {row['stdev']}, num_called_alleles == {row['num_called_alleles']}"))
    if len(allele_set) > 1 and row["stdev"] == 0:
        errors.append((f"{dataset_name}: len(allele_set) > 1 and stdev == 0", f"{row['locus_id']}: len({allele_set}) == {len(allele_set)}, stdev == {row['stdev']}"))

    return errors

def parse_allele_histograms_from_tsv(tsv_path, dataset_name):
    """Parse the tenk10k TSV file line by line and create a lookup dictionary."""

    lookup = {}
    error_counter = collections.Counter()
    error_examples = collections.defaultdict(list)

    df = pd.read_table(tsv_path, dtype={"locus_id": str})
    df["allele_size_histogram"] = df["allele_size_histogram"].fillna("")
    df["canonical_motif"] = df["motif"].apply(compute_canonical_motif)
    df_grouped_by_motif = df.groupby("canonical_motif")
    df["stdev_rank_by_motif"] = df_grouped_by_motif["stdev"].rank(ascending=False)
    df["stdev_rank_total_number_by_motif"] = df_grouped_by_motif["locus_id"].transform("count")

    total_records = len(df)
    for _, row in tqdm.tqdm(df.iterrows(), unit=" loci", unit_scale=True, total=total_records):
        locus_id = row["locus_id"]
        lookup[locus_id] = {
            "locus_id": locus_id,
            "allele_size_histogram": row["allele_size_histogram"],
            "min_allele": int(row["min_allele"]),
            "mode_allele": int(row["mode_allele"]),
            "stdev": float(row["stdev"]),
            "median": float(row["median"]),
            "99th_percentile": float(row["99th_percentile"]),
            "max_allele": int(row["max_allele"]),
            "unique_alleles": int(row["unique_alleles"]),
            "num_called_alleles": int(row["num_called_alleles"]),
            "stdev_rank_by_motif": int(row["stdev_rank_by_motif"]),
            "stdev_rank_total_number_by_motif": int(row["stdev_rank_total_number_by_motif"]),
        }
        if not pd.isna(row["biallelic_histogram"]):
            lookup[locus_id]["biallelic_histogram"] = row["biallelic_histogram"]
        errors = population_data_sanity_check(lookup[locus_id], dataset_name)
        for error_type, error_detail in errors:
            error_counter[error_type] += 1
            if len(error_examples[error_type]) < 100:
                error_examples[error_type].append(error_detail)

    # Print error summary
    if error_counter:
        print(f"\n{dataset_name} sanity check errors:")
        for error_type, count in error_counter.most_common():
            print(f"  {count:,d} out of {total_records:,d} ({count/total_records:.1%}): {error_type}")
            for example in error_examples[error_type]:
                print(f"      {example}")

    return lookup


def parse_AoU1027_data_from_tsv(tsv_path):
    """Parse the AoU1027 TSV file line by line and create a lookup dictionary."""
    lookup = {}
    error_counter = collections.Counter()
    error_examples = collections.defaultdict(list)
    total_records = 0

    with gzip.open(tsv_path, "rt") as f:
        header = f.readline().rstrip("\n").split("\t")
        col_indices = {col: i for i, col in enumerate(header)}
        for line in tqdm.tqdm(f, unit=" lines", unit_scale=True):
            total_records += 1
            fields = line.rstrip("\n").split("\t")
            locus_id = fields[col_indices["TRID2"]]
            motif_size = len(fields[col_indices["longestPureSegmentMotif"]])
            lookup[locus_id] = {
                'locus_id': locus_id,
                "min_allele": int(float(fields[col_indices["0thPercentile"]]))//motif_size,
                "mode_allele": int(float(fields[col_indices["Mode"]]))//motif_size,  # mode allele is in the table as number of repeats
                "stdev": float(fields[col_indices["Stdev"]])/motif_size,
                "median": float(fields[col_indices["50thPercentile"]])//motif_size,
                "99th_percentile": float(fields[col_indices["99thPercentile"]])//motif_size,
                "max_allele": int(float(fields[col_indices["100thPercentile"]]))//motif_size,
                "unique_alleles": int(fields[col_indices["numAlleles"]]),
                "num_called_alleles": int(fields[col_indices["numCalledAlleles"]]),

                "stdev_rank_by_motif": int(fields[col_indices["StdevRankByMotif"]]),
                "stdev_rank_total_number_by_motif": int(fields[col_indices["StdevRankTotalNumberByMotif"]]),
                #"combined_lps_stdev": float(fields[col_indices["combinedLPSStdev"]]),
                #"expected_lps_stdev": float(fields[col_indices["expectedCombinedLPSStdev"]]),
                "oe_length": float(fields[col_indices["OE_len"]]) if fields[col_indices["OE_len"]] != "" else None,
                "oe_length_percentile": float(fields[col_indices["OE_len_percentile"]]) if fields[col_indices["OE_len_percentile"]] != "" else None,
            }

            errors = population_data_sanity_check(lookup[locus_id], "AoU1027")
            for error_type, error_detail in errors:
                error_counter[error_type] += 1
                if len(error_examples[error_type]) < 25:
                    error_examples[error_type].append(error_detail)

    # Print error summary
    if error_counter:
        print(f"\nAoU1027 sanity check errors:")
        for error_type, count in error_counter.most_common():
            print(f"  {count:,d} out of {total_records:,d} ({count/total_records:.1%}): {error_type}")
            for example in error_examples[error_type]:
                print(f"      {example}")

    return lookup

def parse_vamos_tsv(vamos_tsv):
    print(f"Parsing {vamos_tsv}")
    vamos_columns_lookup = {}
    fopen = gzip.open if vamos_tsv.endswith("gz") else open
    with fopen(vamos_tsv, "rt") as f:
        header = f.readline().rstrip("\n").split("\t")
        for line in tqdm.tqdm(f, unit=" lines", unit_scale=True):
            fields = line.rstrip("\n").split("\t")
            data = dict(zip(header, fields))
            reference_region = data.pop("ReferenceRegion")
            vamos_columns_lookup[reference_region] = data

    return vamos_columns_lookup


## Parse population allele frequency data from different sources
# check that files exist
for name in "reference_fasta", "tenk10k_tsv", "hprc256_tsv", "aou1027_tsv", "vamos_tsv":
    path = getattr(args, name)
    if path.startswith("http") or path.startswith("gs:"):
        continue

    path = os.path.expanduser(path)
    setattr(args, name, path)
    if not os.path.isfile(path):
        parser.error(f"ERROR: {name} not found: {path}")


tenk10k_lookup = {}
if args.tenk10k_tsv:
    print(f"Parsing Tenk10k data from {args.tenk10k_tsv}")
    tenk10k_lookup = parse_allele_histograms_from_tsv(args.tenk10k_tsv, "TenK10K")
    print(f"Parsed {len(tenk10k_lookup):,d} records from the tenk10k table: {args.tenk10k_tsv}")
    
hprc256_lookup = {}
if args.hprc256_tsv:
    print(f"Parsing HPRC256 data from {args.hprc256_tsv}")
    hprc256_lookup = parse_allele_histograms_from_tsv(args.hprc256_tsv, "HPRC256")
    print(f"Parsed {len(hprc256_lookup):,d} records from the hprc256 table: {args.hprc256_tsv}")

aou1027_lookup = {}
if args.aou1027_tsv:
    print(f"Parsing AoU1027 data from {args.aou1027_tsv}")
    aou1027_lookup = parse_AoU1027_data_from_tsv(args.aou1027_tsv)
    print(f"Parsed {len(aou1027_lookup):,d} records from the AoU1027 table: {args.aou1027_tsv}")

if hprc256_lookup and aou1027_lookup:
    # compute correlation between medians, stdevs, 
    shared_keys = set(hprc256_lookup.keys()) & set(aou1027_lookup.keys())
    for column in "min_allele", "mode_allele", "stdev", "median", "99th_percentile", "max_allele", "unique_alleles":
        pearsonr, _ = scipy.stats.pearsonr(
            [hprc256_lookup[k][column] for k in shared_keys],
            [aou1027_lookup[k][column] for k in shared_keys])
        if pearsonr < 0.66:
            print(f"WARNING: pearson correlation coefficient (R) between the {column} values of HPRC256 and AoU1027 is only: {pearsonr}")
        else:
            print(f"Correlation between the {column} at {len(shared_keys):,d} TR loci in HPRC256 and AoU1027 is {pearsonr:0.2f}")

vamos_columns_lookup = parse_vamos_tsv(args.vamos_tsv)

## Parse known disease-associated loci from gnomAD, STRchive and STRipy
if not args.known_disease_associated_loci:
    args.known_disease_associated_loci = [
        "https://raw.githubusercontent.com/broadinstitute/str-analysis/refs/heads/main/str_analysis/variant_catalogs/variant_catalog_without_offtargets.GRCh38.json",
        "https://storage.googleapis.com/tandem-repeat-catalog/v2.0/known_disease_associated_loci_v2.loci_to_include_in_catalog.bed.gz",
    ]

known_disease_associated_loci = {}
for catalog_path in args.known_disease_associated_loci:
    print(f"Parsing known disease-associated loci from {catalog_path}")

    for record in get_variant_catalog_iterator(catalog_path):
        # Get reference region, normalizing to remove 'chr' prefix
        reference_region = record.get("MainReferenceRegion") or record.get("ReferenceRegion")
        reference_region = reference_region.replace("chr", "")

        # Skip if we already have this locus (first source wins)
        if reference_region in known_disease_associated_loci:
            continue

        # For JSON files with disease info, require Diseases field
        # For BED files (no Diseases field), include all loci
        if "Diseases" not in record and ".json" in catalog_path:
            continue

        # Extract motif from LocusStructure if RepeatUnit not present
        if "RepeatUnit" not in record and "LocusStructure" in record:
            motif_match = re.match(r"^[(]([A-Z]+)[)][+*]", record["LocusStructure"])
            if motif_match:
                record["RepeatUnit"] = motif_match.group(1)

        # For BED files, add placeholder Diseases field
        if "Diseases" not in record:
            record["Diseases"] = [{"Symbol": record["LocusId"]}]

        record["MainReferenceRegion"] = reference_region
        known_disease_associated_loci[reference_region] = record

    print(f"  Total unique loci so far: {len(known_disease_associated_loci)}")

known_disease_associated_locus_ids = {x["LocusId"] for x in known_disease_associated_loci.values()}
print(f"Parsed {len(known_disease_associated_loci)} unique known disease-associated loci from {len(args.known_disease_associated_loci)} source(s):")
print(", ".join(sorted(known_disease_associated_locus_ids)))

strchive_data_json = requests.get("https://raw.githubusercontent.com/dashnowlab/STRchive/refs/heads/main/data/STRchive-loci.json").json()
strchive_id_lookup = {}
for locus in strchive_data_json:
    strchive_gene = locus["gene"].upper()
    strchive_locus_id = locus["id"].upper()
    if strchive_gene in strchive_id_lookup:
        print(f"WARNING: {strchive_gene} already has an ID in strchive_gene_to_id: {strchive_id_lookup[strchive_gene]}")
    strchive_id_lookup[strchive_gene] = strchive_locus_id

strchive_id_lookup["ARX_1"] = strchive_id_lookup["ARX"]
strchive_id_lookup["ARX_2"] = strchive_id_lookup["ARX"]
strchive_id_lookup["HOXA13_1"] = strchive_id_lookup["HOXA13"]
strchive_id_lookup["HOXA13_2"] = strchive_id_lookup["HOXA13"]
strchive_id_lookup["HOXA13_3"] = strchive_id_lookup["HOXA13"]
del strchive_id_lookup["ARX"]
del strchive_id_lookup["HOXA13"]

stripy_id_lookup = {}
# Compute STRipy urls
for locus_id in known_disease_associated_locus_ids:
    stripy_name = locus_id
    stripy_url = f"https://stripy.org/database/{stripy_name}"
    r = requests.get(stripy_url)
    if r.ok and "invalid locus" not in r.content.decode("UTF-8").lower():
        stripy_id_lookup[locus_id] = stripy_name
    else:
        print(f"WARNING: STRipy page not found for {locus_id}")

for locus_id in set(strchive_id_lookup.keys()) - {x["LocusId"] for x in known_disease_associated_loci.values()}:
    print(f"WARNING: {locus_id} is in STRchive but not in {args.known_disease_associated_loci}")

known_disease_associated_loci_interval_trees = collections.defaultdict(intervaltree.IntervalTree)
for reference_region, locus_info in known_disease_associated_loci.items():
    if locus_info["LocusId"] in strchive_id_lookup:
        locus_info["STRchiveId"] = strchive_id_lookup[locus_info["LocusId"]]
    else:
        print(f"WARNING: {locus_info['LocusId']} is not in STRchive")

    if locus_info["LocusId"] in stripy_id_lookup:
        locus_info["STRipyId"] = stripy_id_lookup[locus_info["LocusId"]]
    else:
        print(f"WARNING: {locus_info['LocusId']} is not in STRipy")

    chrom, start_0based, end_1based = parse_interval(reference_region)
    known_disease_associated_loci_interval_trees[chrom].addi(start_0based, end_1based, data=locus_info)

    # drop all keys from  locus_info except LocusId, STRchiveId, and Diseases
    known_disease_associated_loci[reference_region] = {
        key: locus_info[key] for key in ["LocusId", "STRchiveId", "STRipyId", "Diseases", "RepeatUnit"] if key in locus_info
    }

## Compute GangSTR catalog lookup

# Example row: chr11	98522293	98522307	3	AAC	AACAACAACAACAAC
gangstr_df = pd.read_table(
    "https://s3.amazonaws.com/gangstr/hg38/genomewide/hg38_ver17.bed.gz",
    names=["chrom", "start_1based", "end_1based", "motif_size", "motif", "seq"]
)
print(f"Parsed {len(gangstr_df):,d} records from the    hg38_ver17.bed.gz GangSTR catalog")

gangstr_interval_set = set()
gangstr_interval_trees = collections.defaultdict(intervaltree.IntervalTree)
for index, row in gangstr_df.iterrows():
    chrom = row["chrom"].replace("chr", "")
    start_0based = row["start_1based"] - 1
    end_1based = row["end_1based"]
    canonical_motif = compute_canonical_motif(row["motif"])
    gangstr_interval_trees[chrom].addi(start_0based, end_1based, data=canonical_motif)
    gangstr_interval_set.add((chrom, start_0based, end_1based, canonical_motif))

## Compute repeat_masker_lookup
repeat_masker_lookup = {}
if args.repeat_masker_lookup_json:
    print(f"Parsing repeat masker lookup from {args.repeat_masker_lookup_json}")
    with gzip.open(args.repeat_masker_lookup_json, "rt") as f:
        repeat_masker_lookup = json.load(f)
    print(f"Parsed {len(repeat_masker_lookup):,d} records from the repeat masker lookup: {args.repeat_masker_lookup_json}")
    # convert to repeat masker lookup entries to string format
    print(f"Converting {len(repeat_masker_lookup):,d} repeat masker lookup entries to string format")
    for locus_id, repeat_masker_intervals in tqdm.tqdm(repeat_masker_lookup.items(), unit=" loci", unit_scale=True, total=len(repeat_masker_lookup)):
        repeat_masker_lookup[locus_id] = ", ".join([f"{interval['repFamily']}:{interval['repName']}" for interval in repeat_masker_intervals])  # {interval['repClass']}:
    print(f"Done converting {len(repeat_masker_lookup):,d} repeat masker lookup entries to string format")

## Compute non-coding annotations lookup
non_coding_annotations_interval_trees = collections.defaultdict(intervaltree.IntervalTree)
if args.non_coding_annotations_bed:
    total_intervals = 0
    with gzip.open(args.non_coding_annotations_bed, "rt") as f:
        for line in f:
            fields = line.rstrip("\n").split("\t")
            chrom = fields[0].replace("chr", "")
            start_0based = int(fields[1])
            end_1based = int(fields[2])
            category = fields[3]
            non_coding_annotations_interval_trees[chrom].addi(start_0based, end_1based, data=category)
            total_intervals += 1
    print(f"Parsed {total_intervals:,d} records from the non-coding annotations file: {args.non_coding_annotations_bed}")



schema = []
for column in BIGQUERY_COLUMNS:
    schema.append(bigquery.SchemaField(column["name"], column["type"], mode=column.get("mode", "NULLABLE")))
field_names = {field.name for field in schema}

# Initialize BigQuery client
client = bigquery.Client(project=PROJECT_ID)

def does_table_exist(table_ref):
    try:
        client.get_table(table_ref)
        return True
    except Exception:
        return False

def insert_with_retries(table_ref, rows_to_insert, batch_size=1000, max_retries=5):
    for i in range(0, len(rows_to_insert), batch_size):
        batch = rows_to_insert[i:i+batch_size]
        retries = 0
        while retries < max_retries:
            try:
                errors = client.insert_rows_json(table_ref, batch)
                if errors:
                    print(f"Encountered errors while inserting rows: {errors}")
                    raise Exception(f"Encountered errors while inserting rows: {errors}")
                break
            except Exception as e:
                print(f"Error inserting batch: {e}. Retrying...")
                retries += 1
                time.sleep(5)
        if retries == max_retries:
            raise Exception(f"Failed to insert batch after {max_retries} retries")

# Create dataset if it doesn't exist
dataset_ref = client.dataset(DATASET_ID)
try:
    client.get_dataset(dataset_ref)
    print(f"BigQuery dataset {DATASET_ID} already exists")
except:
    dataset = bigquery.Dataset(dataset_ref)
    dataset.location = "US-CENTRAL1"
    client.create_dataset(dataset)
    print(f"Created dataset {DATASET_ID}")

## Create BigQuery table
new_table_id = f"{TABLE_ID}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
new_table_ref = dataset_ref.table(new_table_id)

if does_table_exist(new_table_ref):
    print(f"ERROR: Table {new_table_id} already exists")
    sys.exit(1)

new_table = bigquery.Table(new_table_ref, schema=schema)
new_table.range_partitioning = bigquery.RangePartitioning(
    field="chrom_index",
    range_=bigquery.PartitionRange(start=1, end=26, interval=1),
)
new_table.clustering_fields = ["start_0based"]


client.create_table(new_table)
print(f"Created table {new_table_id}")

## Read and load TRExplorer catalog
print(f"Reading catalog from {args.catalog_path}")
is_gzipped = args.catalog_path.endswith("gz")
if args.catalog_path.startswith("http"):
    response = requests.get(args.catalog_path)
    catalog = get_json_iterator(response.content, is_gzipped)
elif os.path.isfile(os.path.expanduser(args.catalog_path)):
    catalog = get_json_iterator(os.path.expanduser(args.catalog_path), is_gzipped)
else:
    parser.error(f"Invalid catalog path: {args.catalog_path}")


# vamos does not support overlapping loci - double-check that none of the loci with IncludeInVamosCatalog=1 overlap each other
vamos_overlap_detector_last_end_coord = collections.defaultdict(int)

chrom_indices = {str(i): i for i in range(1, 23)}
chrom_indices.update({"X": 23, "Y": 24, "M": 25, "MT": 25})

fasta_obj = pyfaidx.Fasta(args.reference_fasta, one_based_attributes=False, as_raw=True, sequence_always_upper=True)

rows_to_insert = []
batch_size = 1000
locus_ids_with_added_disease_info = set()

previous_chrom = None
chrom_fasta_obj = None

counters = collections.Counter()
for record_i, record in tqdm.tqdm(enumerate(catalog), unit=" records", unit_scale=True):
    if args.n and record_i >= args.n:
        break

    chrom, start_0based, end_1based = parse_interval(record["ReferenceRegion"])
    record["chrom"] = chrom.replace("chr", "").upper()
    record["chrom_index"] = chrom_indices[record["chrom"]]
    record["start_0based"] = start_0based
    record["end_1based"] = end_1based
    record["ReferenceRegionSize"] = end_1based - start_0based
    record["MotifSize"] = len(record["CanonicalMotif"])
    
    # get reference sequence and flanks
    flanking_sequence_size = max(50, record["MotifSize"] * 5)
    if previous_chrom != chrom:
        previous_chrom = chrom
        print(f"Loading reference genome sequence for {chrom}")
        chrom_fasta_obj = str(fasta_obj[chrom])

    record["ReferenceRepeatSequence"] = chrom_fasta_obj[start_0based:end_1based]
    record["ReferenceLeftFlank"] = chrom_fasta_obj[start_0based - flanking_sequence_size:start_0based]
    record["ReferenceRightFlank"] = chrom_fasta_obj[end_1based:end_1based + flanking_sequence_size]

    if record.get("StdevFromT2TAssemblies"):
        record["StdevFromT2TAssemblies"] = round(record["StdevFromT2TAssemblies"], 3)

    if record.get("StdevFromIllumina174k"):
        record["StdevFromIllumina174k"] = round(record["StdevFromIllumina174k"], 3)

    motif_match = re.match(r"^[(]([A-Z]+)[)][+*]", record["LocusStructure"])
    record["ReferenceMotif"] = motif_match.group(1) if motif_match else None

    catalog_reference_region = record["ReferenceRegion"].replace("chr", "")
    if catalog_reference_region in known_disease_associated_loci:
        known_locus_info = known_disease_associated_loci[catalog_reference_region]
        locus_ids_with_added_disease_info.add(known_locus_info["LocusId"])
        record["DiseaseInfo"] = json.dumps(known_locus_info)
        #print(f"Added disease info for locus #{len(locus_ids_with_added_disease_info)}:", known_locus_info["LocusId"])
    else:
        chrom, start_0based, end_1based = parse_interval(catalog_reference_region)
        for overlapping_interval in known_disease_associated_loci_interval_trees[chrom].overlap(start_0based, end_1based):
            known_locus_info = overlapping_interval.data
            known_locus_canonical_motif = compute_canonical_motif(known_locus_info["RepeatUnit"])
            catalog_record_canonical_motif = compute_canonical_motif(record["ReferenceMotif"])

            if known_locus_canonical_motif == catalog_record_canonical_motif and overlapping_interval.overlap_size(start_0based, end_1based) > len(catalog_record_canonical_motif):
                record["DiseaseInfo"] = json.dumps(known_locus_info)
                locus_ids_with_added_disease_info.add(known_locus_info["LocusId"])
                print()
                print(f"Added disease info for overlapping locus #{len(locus_ids_with_added_disease_info)}:", known_locus_info["LocusId"])
                print(f"    Known locus definition: {known_locus_info['ReferenceRegion']} {known_locus_info['RepeatUnit']}")
                print(f"  Catalog locus definition: {record['ReferenceRegion']} {record['ReferenceMotif']}")
                break

    if record.get("StdevFromIllumina174k") and not record.get("AlleleFrequenciesFromIllumina174k"):
        print("WARNING: StdevFromIllumina174k is present but AlleleFrequenciesFromIllumina174k is missing in record:", pformat(record, indent=4))

    #if record.get("StdevFromT2TAssemblies") and not record.get("AlleleFrequenciesFromT2TAssemblies"):
    #    This happens where loci in source catalogs overlapped
    #    print("WARNING: StdevFromT2TAssemblies is present but AlleleFrequenciesFromT2TAssemblies is missing in record:", pformat(record, indent=4))

    # Add id field
    record["id"] = record_i + 1

    if record["LocusId"] in tenk10k_lookup:
        counters["rows_with_tenk10k_data"] += 1
        tenk10k_record = tenk10k_lookup[record["LocusId"]]
        record["TenK10K_AlleleHistogram"] = tenk10k_record["allele_size_histogram"]
        record["TenK10K_BiallelicHistogram"] = tenk10k_record.get("biallelic_histogram")
        record["TenK10K_MinAllele"] = tenk10k_record["min_allele"]
        record["TenK10K_ModeAllele"] = tenk10k_record["mode_allele"]
        record["TenK10K_Stdev"] = tenk10k_record["stdev"]
        record["TenK10K_Median"] = tenk10k_record["median"]
        record["TenK10K_99thPercentile"] = tenk10k_record["99th_percentile"]
        record["TenK10K_MaxAllele"] = tenk10k_record["max_allele"]
        record["TenK10K_UniqueAlleles"] = tenk10k_record["unique_alleles"]
        record["TenK10K_NumCalledAlleles"] = tenk10k_record["num_called_alleles"]
        record["TenK10K_StdevRankByMotif"] = tenk10k_record["stdev_rank_by_motif"]
        record["TenK10K_StdevRankTotalNumberByMotif"] = tenk10k_record["stdev_rank_total_number_by_motif"]

    if record["LocusId"] in hprc256_lookup:
        counters["rows_with_hprc256_data"] += 1
        hprc256_record = hprc256_lookup[record["LocusId"]]
        record["HPRC256_AlleleHistogram"] = hprc256_record["allele_size_histogram"]
        record["HPRC256_BiallelicHistogram"] = hprc256_record.get("biallelic_histogram")
        record["HPRC256_MinAllele"] = hprc256_record["min_allele"]
        record["HPRC256_ModeAllele"] = hprc256_record["mode_allele"]
        record["HPRC256_Stdev"] = hprc256_record["stdev"]
        record["HPRC256_Median"] = hprc256_record["median"]
        record["HPRC256_99thPercentile"] = hprc256_record["99th_percentile"]
        record["HPRC256_MaxAllele"] = hprc256_record["max_allele"]
        record["HPRC256_UniqueAlleles"] = hprc256_record["unique_alleles"]
        record["HPRC256_NumCalledAlleles"] = hprc256_record["num_called_alleles"]
        record["HPRC256_StdevRankByMotif"] = hprc256_record["stdev_rank_by_motif"]
        record["HPRC256_StdevRankTotalNumberByMotif"] = hprc256_record["stdev_rank_total_number_by_motif"]

    if record["LocusId"] in aou1027_lookup:
        counters["rows_with_aou1027_data"] += 1
        aou1027_record = aou1027_lookup[record["LocusId"]]
        record["AoU1027_MinAllele"] = aou1027_record["min_allele"]
        record["AoU1027_ModeAllele"] = aou1027_record["mode_allele"]
        record["AoU1027_Stdev"] = aou1027_record["stdev"]
        record["AoU1027_Median"] = aou1027_record["median"]
        record["AoU1027_99thPercentile"] = aou1027_record["99th_percentile"]
        record["AoU1027_MaxAllele"] = aou1027_record["max_allele"]
        record["AoU1027_UniqueAlleles"] = aou1027_record["unique_alleles"]
        record["AoU1027_NumCalledAlleles"] = aou1027_record["num_called_alleles"]
        record["AoU1027_StdevRankByMotif"] = aou1027_record["stdev_rank_by_motif"]
        record["AoU1027_StdevRankTotalNumberByMotif"] = aou1027_record["stdev_rank_total_number_by_motif"]

        #record["AoU1027_CombinedLPSStdev"] = aou1027_record["combined_lps_stdev"]
        #record["AoU1027_ExpectedLPSStdev"] = aou1027_record["expected_lps_stdev"]
        record["AoU1027_OE_Length"] = aou1027_record["oe_length"]
        record["AoU1027_OE_LengthPercentile"] = aou1027_record["oe_length_percentile"]


    gangstr_interval_tree = gangstr_interval_trees[record["chrom"]]
    gangstr_catalog_overlap = gangstr_interval_tree.overlap(record["start_0based"], record["end_1based"])
    for gangstr_interval in gangstr_catalog_overlap:
        gangstr_canonical_motif = gangstr_interval.data
        if gangstr_canonical_motif == record["CanonicalMotif"] and (
            (
                gangstr_interval.begin == record["start_0based"]
                and gangstr_interval.end == record["end_1based"]
            ) or (
                gangstr_interval.overlap_size(record["start_0based"], record["end_1based"]) >= 2*record["MotifSize"]
            )
        ):
            record["FoundInGangSTRCatalog"] = 1
            # remove all intervals in the GangSTR catalog that overlap and match TRExplorer interval
            key = (record["chrom"], gangstr_interval.begin, gangstr_interval.end, gangstr_canonical_motif)
            if key in gangstr_interval_set:
                gangstr_interval_set.remove(key)

    if record["ReferenceRegion"] in vamos_columns_lookup:
        counters["rows_with_vamos_data"] += 1
        vamos_data = vamos_columns_lookup[record["ReferenceRegion"]]
        for c in "VamosUniqueMotifs", "VamosEfficientMotifs", "VamosMotifFrequencies":
            if vamos_data[c] is not None and vamos_data[c] != "":
                record[c] = vamos_data[c]
        for c in "VamosNumUniqueMotifs", "IncludeInVamosCatalog":
            if vamos_data[c] is not None and vamos_data[c] != "":
                record[c] = int(vamos_data[c])

        if int(vamos_data["IncludeInVamosCatalog"]) == 1:
            # since Vamos doesn't support overlapping loci, double-check that this locus doesn't overlap any other
            # loci that are to be included in the Vamos catalog
            if vamos_overlap_detector_last_end_coord[chrom] > start_0based:
                raise ValueError(f"Overlapping vamos catalog locus detected for new locus: {chrom}:{start_0based}-{end_1based}")
            vamos_overlap_detector_last_end_coord[chrom] = end_1based

    if record["LocusId"] in repeat_masker_lookup:
        counters["rows_with_repeat_masker_intervals"] += 1
        record["RepeatMaskerIntervals"] = repeat_masker_lookup[record["LocusId"]]


    non_coding_annotations_interval_tree = non_coding_annotations_interval_trees[record["chrom"]]
    non_coding_annotations_catalog_overlap = non_coding_annotations_interval_tree.overlap(record["start_0based"], record["end_1based"])
    for non_coding_annotations_interval in non_coding_annotations_catalog_overlap:
        non_coding_annotations_category = non_coding_annotations_interval.data
        
        if "NonCodingAnnotations" not in record:
            record["NonCodingAnnotations"] = non_coding_annotations_category
        else:
            previous_annotations = [a.strip() for a in record["NonCodingAnnotations"].split(",")]
            if non_coding_annotations_category not in previous_annotations:
                previous_annotations.append(non_coding_annotations_category)
                record["NonCodingAnnotations"] = ", ".join(sorted(previous_annotations))

    counters["total_rows"] += 1

    # Convert any None values to None (BigQuery will handle NULL)
    row = {k: v for k, v in record.items() if k in field_names}
    rows_to_insert.append(row)
    if len(rows_to_insert) >= batch_size:
        insert_with_retries(new_table_ref, rows_to_insert)
        rows_to_insert = []

if rows_to_insert:
    insert_with_retries(new_table_ref, rows_to_insert)

if len(locus_ids_with_added_disease_info) != len(known_disease_associated_locus_ids):
    print(f"WARNING: {len(known_disease_associated_locus_ids) - len(locus_ids_with_added_disease_info)} out of {len(known_disease_associated_locus_ids)} known disease-associated loci were not found in the catalog. "
           "Missing LocusIds:", ", ".join(known_disease_associated_locus_ids - locus_ids_with_added_disease_info))

for html_path in "../website/header_template.html", "../index.html", "../locus.html":
    print(f"Update TABLE_ID to '{new_table_id}' in {html_path}")
    with open(html_path, "r") as f:
        html_content = f.read()

    html_content = re.sub(
        r"const TABLE_ID[\s]*=[\s]*'catalog[^']*'",
        f"const TABLE_ID = '{new_table_id}'",
        html_content)

    with open(html_path, "wt") as f:
        f.write(html_content)

print("Done!")

# print any GangSTR loci that unexpectedly were not found in the TRExplorer catalog
if len(gangstr_interval_set) > 0:
    print_N = 100
    for record_i, (chrom, start_0based, end_1based, canonical_motif) in enumerate(sorted(gangstr_interval_set)):
        if record_i >= print_N:
            print(f" ... and {len(gangstr_interval_set) - print_N:,d} other loci")
            break
        print(f"{record_i+1:3d}: GangSTR locus not found in TRExplorer catalog: {chrom}:{start_0based}-{end_1based}  {canonical_motif}")
    print(f"WARNING: {len(gangstr_interval_set):,d} GangSTR catalog loci were not found in the TRExplorer catalog")

print("\nCounters:")
total_rows = counters["total_rows"]
for key, count in sorted(counters.items(), key=lambda x: x[1], reverse=True):
    if key == "total_rows":
        print(f"{count:10,d}  {key}")
    else:
        print(f"{count:10,d} ( {count/total_rows*100:.1f}%)  {key}")
